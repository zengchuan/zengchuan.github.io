[{"id":"201603220000000588","sortNo":1,"contentFormatId":"1","contentTxt":"Hadoop是一个分布式系统基础架构，由Apache基金会开发。用户可以在不了解分布式底层细节的情况下，开发分布式程序。充分利用集群的威力高速运算和存储。简单地说来，Hadoop是一个可以更容易开发和运行处理大规模数据的软件平台。hadoop主要的一些特点：","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000589","sortNo":2,"contentFormatId":"1","contentTxt":"1、扩容能力（Scalable）：能可靠地（reliably）存储和处理千兆字节（PB）数据。","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000590","sortNo":3,"contentFormatId":"1","contentTxt":"2、成本低（Economical）：可以通过普通机器组成的服务器群来分发以及处理数据。这些服务器群总计可达数千个节点。","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000591","sortNo":4,"contentFormatId":"1","contentTxt":"3、高效率（Efficient）：通过分发数据，hadoop可以在数据所在的节点上并行地（parallel）处理它们，这使得处理非常的快速。","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000592","sortNo":5,"contentFormatId":"1","contentTxt":"4、可靠性（Reliable）：hadoop能自动地维护数据的多份复制，并且在任务失败后能自动地重新部署（redeploy）计算任务。","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000593","sortNo":6,"contentFormatId":"1","contentTxt":"Hadoop实现了一个分布式文件系统（Hadoop Distributed File System），简称HDFS。HDFS有着高容错性（fault-tolerent）的特点，并且设计用来部署在低廉的（low-cost）硬件上。而且它提供高传输率（high throughput）来访问应用程序的数据，适合那些有着超大数据集（large data set）的应用程序。HDFS放宽了（relax）POSIX的要求（requirements）这样可以流的形式访问（streaming access）文件系统中的数据。","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000594","sortNo":7,"contentFormatId":"1","contentTxt":"MapReduce将应用程序的工作分解成很多小的工作小块(small blocks of work)。HDFS为了做到可靠性（reliability）创建了多份数据块（data blocks）的复制（replicas），并将它们放置在服务器群的计算节点中（compute nodes），MapReduce就可以在它们所在的节点上处理这些数据了。（体现一种分而治之的思想）。","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000651","sortNo":8,"contentFormatId":"1","contentTxt":"分布式文件系统HDFS","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000652","sortNo":9,"contentFormatId":"2","contentTxt":null,"contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000653","sortNo":10,"contentFormatId":"1","contentTxt":"Hadoop分布式文件系统(HDFS)被设计成适合运行在通用硬件(commodity hardware)上的分布式文件系统。有以下特点：","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000654","sortNo":11,"contentFormatId":"1","contentTxt":"1、HDFS是一个高度容错性的系统，适合部署在廉价的机器上。","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000655","sortNo":12,"contentFormatId":"1","contentTxt":"2、HDFS能提供高吞吐量的数据访问，非常适合大规模数据集上的应用。","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000656","sortNo":13,"contentFormatId":"1","contentTxt":"3、HDFS放宽了一部分POSIX约束，来实现流式读取文件系统数据的目的。","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000657","sortNo":14,"contentFormatId":"1","contentTxt":"HDFS采用master/slave架构。一个HDFS集群是由一个Namenode和一定数目的Datanodes组成。Namenode是一个中心服务器，负责管理文件系统的名字空间(namespace)以及客户端对文件的访问。集群中的Datanode一般是一个节点一个，负责管理它所在节点上的存储。HDFS暴露了文件系统的名字空间，用户能够以文件的形式在上面存储数据。从内部看，一个文件其实被分成一个或多个数据块，这些块存储在一组Datanode上。Namenode执行文件系统的名字空间操作，比如打开、关闭、重命名文件或目录。它也负责确定数据块到具体Datanode节点的映射。Datanode负责处理文件系统客户端的读写请求。在Namenode的统一调度下进行数据块的创建、删除和复制。","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000658","sortNo":15,"contentFormatId":"1","contentTxt":"Map/Reduce","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000659","sortNo":16,"contentFormatId":"2","contentTxt":null,"contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000660","sortNo":17,"contentFormatId":"1","contentTxt":"一个Map/Reduce 作业（job） 通常会把输入的数据集切分为若干独立的数据块，由 map任务（task）以完全并行的方式处理它们。框架会对map的输出先进行排序， 然后把结果输入给reduce任务。通常作业的输入和输出都会被存储在文件系统中。 整个框架负责任务的调度和监控，以及重新执行已经失败的任务。","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000661","sortNo":18,"contentFormatId":"1","contentTxt":"hadoop生态系统","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000662","sortNo":19,"contentFormatId":"2","contentTxt":null,"contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000663","sortNo":20,"contentFormatId":"1","contentTxt":"Hadoop Common: 从0.21开始HDFS和MapReduce被分离为独立的子项目，其余内容为Hadoop Common\u000B。","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000664","sortNo":21,"contentFormatId":"1","contentTxt":"HDFS: Hadoop Distributed File System","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000665","sortNo":22,"contentFormatId":"1","contentTxt":"MapReduce：并行计算框架，0.20前使用 org.apache.hadoop.mapred 旧接口，0.20版本开始引入org.apache.hadoop.mapreduce的新API","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000666","sortNo":23,"contentFormatId":"1","contentTxt":"HBase: 类似Google BigTable的分布式NoSQL列数据库。（HBase 和 Avro 已经于2010年5月成为顶级 Apache 项目[1]）","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000667","sortNo":24,"contentFormatId":"1","contentTxt":"Hive：数据仓库工具，由Facebook贡献。","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000668","sortNo":25,"contentFormatId":"1","contentTxt":"Zookeeper：分布式锁设施，提供类似Google Chubby的功能，由Facebook贡献。","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000669","sortNo":26,"contentFormatId":"1","contentTxt":"Avro：新的数据序列化格式与传输工具，将逐步取代Hadoop原有的IPC机制。","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000670","sortNo":27,"contentFormatId":"1","contentTxt":"HBase数据模型","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000671","sortNo":28,"contentFormatId":"2","contentTxt":null,"contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000672","sortNo":29,"contentFormatId":"1","contentTxt":"Row Key: 行键，Table的主键，Table中的记录按照Row Key排序。","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000673","sortNo":30,"contentFormatId":"1","contentTxt":"Timestamp: 时间戳，每次数据操作对应的时间戳，可以看作是数据的version number。","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000674","sortNo":31,"contentFormatId":"1","contentTxt":"Column Family：列簇，Table在水平方向有一个或者多个Column Family组成，一个Column Family中可以由任意多个Column组成，即Column Family支持动态扩展，无需预先定义。","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000675","sortNo":32,"contentFormatId":"1","contentTxt":"Column的数量以及类型，所有Column均以二进制格式存储，用户需要自行进行类型转换。","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000676","sortNo":33,"contentFormatId":"1","contentTxt":"HBase系统架构","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000677","sortNo":34,"contentFormatId":"2","contentTxt":null,"contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000678","sortNo":35,"contentFormatId":"1","contentTxt":"Client","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000679","sortNo":36,"contentFormatId":"1","contentTxt":"包含访问hbase的接口，client维护着一些cache来加快对hbase的访问，比如regione的位置信息。","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000680","sortNo":37,"contentFormatId":"1","contentTxt":"Zookeeper","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000681","sortNo":38,"contentFormatId":"1","contentTxt":"1、保证任何时候，集群中只有一个master\u000B。","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000682","sortNo":39,"contentFormatId":"1","contentTxt":"2、存贮所有Region的寻址入口。","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000683","sortNo":40,"contentFormatId":"1","contentTxt":"3、实时监控Region Server的状态，将Region server的上线和下线信息实时通知给Master\u000B。","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000684","sortNo":41,"contentFormatId":"1","contentTxt":"4、存储Hbase的schema,包括有哪些table，每个table有哪些column family\u000B。","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000685","sortNo":42,"contentFormatId":"1","contentTxt":"Master","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000686","sortNo":43,"contentFormatId":"1","contentTxt":"1、为Region server分配region","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000687","sortNo":44,"contentFormatId":"1","contentTxt":"2、负责region server的负载均衡","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000688","sortNo":45,"contentFormatId":"1","contentTxt":"3、发现失效的region server并重新分配其上的region","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000689","sortNo":46,"contentFormatId":"1","contentTxt":"4、GFS上的垃圾文件回收","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000690","sortNo":47,"contentFormatId":"1","contentTxt":"5、处理schema更新请求","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000691","sortNo":48,"contentFormatId":"1","contentTxt":"Region Server","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000692","sortNo":49,"contentFormatId":"1","contentTxt":"1、Region server维护Master分配给它的region，处理对这些region的IO请求","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000693","sortNo":50,"contentFormatId":"1","contentTxt":"2、Region server负责切分在运行过程中变得过大的region","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000694","sortNo":51,"contentFormatId":"1","contentTxt":"可以看到，client访问hbase上数据的过程并不需要master参与（寻址访问zookeeper和region server，数据读写访问regione server），master仅仅维护者table和region的元数据信息，负载很低。","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000695","sortNo":52,"contentFormatId":"1","contentTxt":"HBase存储格式","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000696","sortNo":53,"contentFormatId":"1","contentTxt":"HBase中的所有数据文件都存储在Hadoop HDFS文件系统上，主要包括上述提出的两种文件类型：","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000697","sortNo":54,"contentFormatId":"1","contentTxt":"1、HFile， HBase中KeyValue数据的存储格式，HFile是Hadoop的二进制格式文件，实际上StoreFile就是对HFile做了轻量级包装，即StoreFile底层就是HFile","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000698","sortNo":55,"contentFormatId":"1","contentTxt":"2、HLog File，HBase中WAL（Write Ahead Log） 的存储格式，物理上是Hadoop的Sequence File","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000699","sortNo":56,"contentFormatId":"1","contentTxt":"Hive系统架构","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000700","sortNo":57,"contentFormatId":"2","contentTxt":null,"contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000701","sortNo":58,"contentFormatId":"1","contentTxt":"UI：用户提交查询和其他的操作。当前系统有一个命令行的接口和基于Web的的GUI。","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000702","sortNo":59,"contentFormatId":"1","contentTxt":"Driver：接受query的组件，该组件实现session的概念，以处理和提供基于JDBC/ODBC执行以及颉取的API。","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000703","sortNo":60,"contentFormatId":"1","contentTxt":"编译器：该组件分析query，在不同的查询块和查询表达式上进行语义分析，并最终通过从metastore中查找表与分区的元信息生成执行计划。","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000704","sortNo":61,"contentFormatId":"1","contentTxt":"Metastore：此组件存储数据仓库里所有的各种表与分区的结构化信息，包括列与列类型信息，序列化器与反序列化器，从而能够读写hdfs中的数据。","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000705","sortNo":62,"contentFormatId":"1","contentTxt":"执行引擎：此组件执行由compiler创建的执行计划。此计划是一个关于阶段的有向无环图。执行引擎管理不同阶段的依赖关系，并在合适的系统组件上执行这些阶段。","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000706","sortNo":63,"contentFormatId":"1","contentTxt":"Hive查询语言：HiveQL是一个类SQL的查询语言。它模仿SQL语法来创建表，读表到数据，并查询表。HiveQL也允许用户嵌入他们自定义的map-reduce脚本。","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000707","sortNo":64,"contentFormatId":"1","contentTxt":"Hbase和Hive的整合","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000708","sortNo":65,"contentFormatId":"1","contentTxt":"Hive与HBase的整合功能的实现是利用两者本身对外的API接口互相进行通信，相互通信主要是依靠hive_hbase-handler.jar工具类。","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000709","sortNo":66,"contentFormatId":"1","contentTxt":"举例：在Hive中创建一张相互关联的表","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000710","sortNo":67,"contentFormatId":"1","contentTxt":"CREATE  EXTERNAL  TABLE  hbase_table(key string,  value string)  STORED  BY  org.apache.hadoop.hive.hbase.HBaseStorageHandler'  WITH  SERDEPROPERTIES  (\"hbase.columns.mapping\" = \"data:1\")  TBLPROPERTIES(\"hbase.table.name\" = \"test\");","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000711","sortNo":68,"contentFormatId":"1","contentTxt":"hbase.columns.mapping指向对应的列族；多列时，data:1，data:2；多列族时，data1:1,data2:1；","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000712","sortNo":69,"contentFormatId":"1","contentTxt":"hbase.table.name指向对应的表；","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000713","sortNo":70,"contentFormatId":"1","contentTxt":"hbase_table_2(key string, value string)，这个是关联表","contentBinary":"","contentPoster":"","contentUrl":null},{"id":"201603220000000714","sortNo":71,"contentFormatId":"1","contentTxt":"这是2012年资料，希望对你了解hadoop有帮助。","contentBinary":"","contentPoster":"","contentUrl":null}]